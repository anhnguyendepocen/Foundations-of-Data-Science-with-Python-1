[
    {
        "front": "supervised learning",
        "back": "Machine learning from a training set that consists of <i>labeled</i> data points."
    }, 
    {
        "front": "unsupervised learning",
        "back": "Machine learning from a set of <i>unlabeled</i> data points."
    },
    {
        "front": "classification",
        "back": "The application of <i>supervised learning</i> to find a function that can map data points to a set of labels." 
    },

    {
        "front": "clustering",
        "back": "The application of <i>unsupervised learning</i> to find a function that can map data points to a set of labels."
    },
    {
        "front": "$K$-means algorithm",
        "back": "An iterative algorithm to partition the points in a data set based on Euclidean distance. Each set of data points is said to be assigned to a <i>cluster</i>, and the cluster is represented by its mean, which is also called its <i>centroid</i>.  The clusters and centroids are refined over the iterations."
    },
    {
        "front": "centroid<br>($K$-means)",
        "back": "The mean of the points assigned to a cluster. In the assignment step of the $K$-Means algorithm, points are assigned to the nearest centroid. "
    },
    {
        "front": "data compression",
        "back": "A data set is replaced by a representation of that data set that uses fewer bits and thus is easier to store or transmit over a channel."
    },
    {
        "front": "lossy data compression",
        "back": "A data set is replaced by a representation of that data set that uses fewer bits but <b>does not allow exact reconstruction</b> of the original data."
    },
    {
        "front": "lossless data compression",
        "back": "A data set is replaced by a representation of that data set that uses fewer bits but that <b>allows exact reconstruction</b> of the original data."
    },
    {
        "front": "vector correlation",
        "back": "The <i>correlation</i> between $n$-vectors $\\mathbf{a}$ and $\\mathbf{b}$ is \\begin{equation*} r=  \\frac{{\\mathbf{a}} \\cdot \\mathbf{{b}} }{ \\Vert\\mathbf{{a}}\\Vert \\,\\Vert\\mathbf{{b}}\\Vert}. \\end{equation*}"
    }, 
    {
        "front": "angle between vectors",
        "back": "The <i>angle</i> between $n$-vectors $\\mathbf{a}$ and $\\mathbf{b}$ is \\begin{equation*} \\theta = \\cos^{-1} \\left( \\frac{{\\mathbf{a}} \\cdot \\mathbf{{b}} }{ \\Vert\\mathbf{{a}}\\Vert \\,\\Vert\\mathbf{{b}}\\Vert}\\right). \\end{equation*}"
    },
    {
        "front": "orthogonal vectors",
        "back": "Vectors $\\mathbf{a}$ and $\\mathbf{b}$ are <i>orthogonal</i> if and only if \\begin{equation*} \\mathbf{a} \\cdot \\mathbf{b} = 0. \\end{equation*}" 
    },
    {
        "front": "scalar projection",
        "back": "Given $n$-vectors $\\mathbf{x}$ and $\\mathbf{y}$, the <i>scalar projection</i> of $\\mathbf{y}$ onto $\\mathbf{x}$ has magnitude equal to the length of the vector in the direction of $\\mathbf{x}$ and is given by \\begin{equation*} \\frac{ \\mathbf{y}  \\cdot \\mathbf{x}}{ \\| \\mathbf{x} \\|}. \\end{equation*}"
    },
    {
        "front": "unit vector",
        "back": "A vector $\\mathbf{x}$ is a <i>unit vector</i> if $\\| \\mathbf{x} \\| =1.$"
    },
    {
        "front": "vector projection",
        "back": "Given $n$-vectors $\\mathbf{x}$ and $\\mathbf{y}$, the <i>vector projection</i> of $\\mathbf{y}$ onto $\\mathbf{x}$ is the vector in the direction of $\\mathbf{x}$ that minimizes the error to $\\mathbf{y}$ and is given by \\begin{equation*} \\frac{\\mathbf{y} \\cdot  \\mathbf{x}_i } { \\left \\Vert \\mathbf{x}_i \\right \\Vert ^2} \\mathbf{x}_i. \\end{equation*}"
    },
    {
        "front": "standard basis",
        "back": "In a Euclidean vector space, the <I>standard basis</I> consists of a set of unique vectors whose components are all zeros except for a single&nbsp;1.<br><b>Example:</b> For $\\mathbb{R}^2$, the standard basis is \\begin{equation*} \\left\\{ \\mathbf{e}_x = \\left[1, 0 \\right], ~ \\mathbf{e}_y = \\left[0, 1 \\right] \\right\\}. \\end{equation*}"
    }, 
    {
        "front": "orthonormal set<br>(of vectors)",
        "back": "A set of vectors $\\left\\{ \\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{n-1}\\right\\}$ is an <I>orthonormal set</I> if and only if: <ol><li> The vectors are all normal; i.e., $\\|\\mathbf{x}_i\\|=1$ for all $i = 0, 1, \\ldots, n-1$.</li><li>  The vectors are mutually orthogonal; i.e., $\\mathbf{x}_i \\cdot \\mathbf{x}_j$ for all $i \\ne j$ in $0, 1, \\ldots, n-1$.</li></ol>"
    },
    {
        "front": "spanning set",
        "back": "A set of vectors $\\mathcal{S}= \\left\\{ \\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{n-1}\\right\\}$ is a <I>spanning set</I> for some set of vectors $\\mathcal{Z}$ if every vector in $\\mathcal{Z}$ can be represented as a linear combination of the vectors in $S$." 
    },
    {
        "front": "minimal<br>(spanning set)",
        "back": "A  spanning set $\\mathcal{S}$ for a set of vectors $\\mathcal{Z}$ is <I>minimal</I> if the removal of any member of $\\mathcal{S}$ would stop it from being a spanning set for $\\mathcal{Z}.$"
    },
    {
        "front": "basis<br>(vector space)",
        "back": "A  set of vectors $\\mathcal{S}$ is a basis for a set of vectors $\\mathcal{Z}$ if $\\mathcal{S}$ is a minimal spanning set for $\\mathcal{Z}$."
    },
    {
        "front": "orthonormal basis<br>(vector space)",
        "back":" A set of vectors $\\mathcal{S}$ is an orthonormal basis for a set of vectors $\\mathcal{Z}$ if 1) $\\mathcal{S}$ is an orthonormal set, and 2) $\\mathcal{S}$ is a minimal spanning set for $\\mathcal{Z}$."
    },
    {
        "front": "dimension<br>(vector space)",
        "back": "Given a set of vectors $\\mathcal{Z}$ in a vector space, the dimension of $\\mathcal{Z}$, denoted $\\operatorname{dim} \\mathcal{Z}$ is the cardinality of a basis for $\\mathcal{Z}$."
    },
    {
        "front": "time-series data",
        "back": "Data that is collected over time, usually at regular intervals. Each data point is associated with a timestamp indicating when the data was collected."
    }, 
    {
        "front": "power spectral density<br>(of a vector)",
        "back": "For a $n$-vector $\\mathbf{x}$ with DFT $\\mathbf{X}$, the power spectral density is the power at each frequency component and is given by \\begin{equation*} P[k] = \\frac{1}{n} \\| \\mathbf{X}[n]\\|^2. \\end{equation*}"
    },
    {
        "front": "signal-space representation",
        "back": "Representation of a set of signals (or vectors) in terms of an orthonormal basis."
    }, 
    {
        "front": "linearly dependent vectors",
        "back": "A set of vectors  are <i>linearly dependent</i> if any of the vectors can be written as a linear combination of the others."
    }, 
    {
        "front": "linearly independent vectors",
        "back": " A set of vectors that is not linearly dependent. None of the vectors can be written as a linear combination of the other vectors."
    }, 
    {
        "front": "rank<br>(of a matrix)",
        "back": "Given a matrix $\\mathbf{M}$, the rank is equal to the maximum number of linearly independent columns (or, equivalently, the maximum number of linearly independent rows). It is denoted by $\\operatorname{rank} \\mathbf{M}$."
    }, 
    {
        "front": "full rank matrix",
        "back": "A matrix with the maximum possible rank, which is equal to the minimum of the number of rows and the number of columns."
    },
    {
        "front": "feature selection",
        "back": "In <i>feature selection</i>, only a subset of the features present in the data is used."
    }, 
    {
        "front": "feature extraction",
        "back": "In <i>feature extraction</i>, a reduced set of features is created, where the new features are functions of the original features."
    }
]
